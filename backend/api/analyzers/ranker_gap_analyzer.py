"""
Ranker Gap Dashboard â€” D1
ë‚´ í”Œë ˆì´ì˜ ëª¨ë“  ì°¨ì›ì„ ë­ì»¤ì™€ ë¹„êµí•˜ëŠ” í†µí•© ëŒ€ì‹œë³´ë“œ.
"ë­ì»¤ê¹Œì§€ì˜ ê±°ë¦¬" ë‹¨ì¼ ì ìˆ˜ (0-100) ì œê³µ.
"""
import math
from typing import List, Dict, Any, Optional


class RankerGapAnalyzer:
    """ë­ì»¤ ê²©ì°¨ ëŒ€ì‹œë³´ë“œ ë¶„ì„ê¸°"""

    # ì¢…í•© ì ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ ë©”íŠ¸ë¦­ ê°€ì¤‘ì¹˜
    METRICS = {
        'avg_rating': {
            'label': 'í‰ê·  í‰ì ',
            'weight': 3.0,
            'unit': '',
            'format': '{:.1f}',
        },
        'win_rate': {
            'label': 'ìŠ¹ë¥ ',
            'weight': 3.0,
            'unit': '%',
            'format': '{:.1f}%',
        },
        'goals_per_game': {
            'label': 'ê²½ê¸°ë‹¹ ë“ì ',
            'weight': 2.5,
            'unit': '',
            'format': '{:.2f}',
        },
        'shot_accuracy': {
            'label': 'ìŠˆíŒ… ì •í™•ë„',
            'weight': 2.0,
            'unit': '%',
            'format': '{:.1f}%',
        },
        'pass_accuracy': {
            'label': 'íŒ¨ìŠ¤ ì„±ê³µë¥ ',
            'weight': 1.5,
            'unit': '%',
            'format': '{:.1f}%',
        },
        'dribble_success_rate': {
            'label': 'ë“œë¦¬ë¸” ì„±ê³µë¥ ',
            'weight': 1.0,
            'unit': '%',
            'format': '{:.1f}%',
        },
    }

    # ë­ì»¤ ë²¤ì¹˜ë§ˆí¬ (ë“±ê¸‰ë³„ â€” ranker-stats API ë°ì´í„° ì—†ëŠ” ê²½ìš° fallback)
    # division codes: 100=ìŠˆí¼ì±”í”¼ì–¸, 200=ì±”í”¼ì–¸, 300=ìŠˆí¼ìœ , 400=ìœ 
    DIVISION_BENCHMARKS = {
        100: {  # ìŠˆí¼ì±”í”¼ì–¸
            'avg_rating': 7.2,
            'win_rate': 65.0,
            'goals_per_game': 2.3,
            'shot_accuracy': 52.0,
            'pass_accuracy': 85.0,
            'dribble_success_rate': 68.0,
        },
        200: {  # ì±”í”¼ì–¸
            'avg_rating': 6.9,
            'win_rate': 60.0,
            'goals_per_game': 2.0,
            'shot_accuracy': 48.0,
            'pass_accuracy': 82.0,
            'dribble_success_rate': 65.0,
        },
        300: {  # ìŠˆí¼ìœ 
            'avg_rating': 6.7,
            'win_rate': 55.0,
            'goals_per_game': 1.7,
            'shot_accuracy': 44.0,
            'pass_accuracy': 79.0,
            'dribble_success_rate': 61.0,
        },
        400: {  # ìœ 
            'avg_rating': 6.5,
            'win_rate': 50.0,
            'goals_per_game': 1.4,
            'shot_accuracy': 40.0,
            'pass_accuracy': 76.0,
            'dribble_success_rate': 57.0,
        },
    }

    # ë­ì»¤ ìŠ¤íƒ¯ í‘œì¤€í¸ì°¨ (fallback)
    RANKER_STD = {
        'avg_rating': 0.5,
        'win_rate': 12.0,
        'goals_per_game': 0.6,
        'shot_accuracy': 10.0,
        'pass_accuracy': 8.0,
        'dribble_success_rate': 12.0,
    }

    @staticmethod
    def _compute_my_aggregate_stats(
        matches: List[Dict],
        player_performances: List[Dict],
    ) -> Dict[str, float]:
        """ë§¤ì¹˜ + í”Œë ˆì´ì–´ í¼í¬ë¨¼ìŠ¤ì—ì„œ ë‚´ í†µí•© ìŠ¤íƒ¯ ì¶”ì¶œ"""
        if not matches:
            return {}

        total = len(matches)
        wins = sum(1 for m in matches if m.get('result') == 'win')
        total_goals = sum(float(m.get('goals_for', 0)) for m in matches)
        total_shots = sum(float(m.get('shots', 1)) for m in matches)
        total_shots_on = sum(float(m.get('shots_on_target', 0)) for m in matches)
        total_pass_rate = sum(float(m.get('pass_success_rate', 0)) for m in matches)

        # Rating from performances
        avg_rating = 6.5  # default
        if player_performances:
            ratings = [float(p.get('rating', 0)) for p in player_performances if float(p.get('rating', 0)) > 0]
            if ratings:
                avg_rating = sum(ratings) / len(ratings)

        # Dribble from performances
        total_dribble_try = sum(int(p.get('dribble_attempts', 0)) for p in player_performances)
        total_dribble_success = sum(int(p.get('dribble_success', 0)) for p in player_performances)

        return {
            'avg_rating': round(avg_rating, 2),
            'win_rate': round(wins / total * 100, 1),
            'goals_per_game': round(total_goals / total, 2),
            'shot_accuracy': round((total_shots_on / max(total_shots, 1)) * 100, 1),
            'pass_accuracy': round(total_pass_rate / total, 1),
            'dribble_success_rate': round(
                (total_dribble_success / max(total_dribble_try, 1)) * 100, 1
            ),
        }

    @staticmethod
    def _extract_ranker_benchmark_from_api(ranker_data_list: List[Dict]) -> Dict[str, Dict]:
        """
        ranker-stats API ì‘ë‹µì—ì„œ ë²¤ì¹˜ë§ˆí¬ ì¶”ì¶œ.
        ì—¬ëŸ¬ ì„ ìˆ˜ì˜ ranker statsë¥¼ í•©ì‚°í•˜ì—¬ ì „ë°˜ì ì¸ ë²¤ì¹˜ë§ˆí¬ ìƒì„±.
        """
        if not ranker_data_list:
            return {}

        # Aggregate all ranker statuses across all player requests
        combined: Dict[str, List[float]] = {
            'avg_rating': [],
            'win_rate': [],
            'goals_per_game': [],
            'shot_accuracy': [],
            'pass_accuracy': [],
            'dribble_success_rate': [],
        }

        for player_entry in ranker_data_list:
            status_list = player_entry.get('status', [])
            if isinstance(status_list, list):
                for s in status_list:
                    combined['avg_rating'].append(float(s.get('spRating', 0)))
                    combined['goals_per_game'].append(float(s.get('goal', 0)))

                    shots = float(s.get('shoot', 0))
                    shots_on = float(s.get('effectiveShoot', 0))
                    combined['shot_accuracy'].append((shots_on / shots * 100) if shots > 0 else 0)

                    pass_try = float(s.get('passTry', 0))
                    pass_success = float(s.get('passSuccess', 0))
                    combined['pass_accuracy'].append((pass_success / pass_try * 100) if pass_try > 0 else 0)

                    dribble_try = float(s.get('dribbleTry', 0))
                    dribble_success = float(s.get('dribbleSuccess', 0))
                    combined['dribble_success_rate'].append(
                        (dribble_success / dribble_try * 100) if dribble_try > 0 else 0
                    )

        benchmark = {}
        for metric, values in combined.items():
            if values:
                n = len(values)
                avg = sum(values) / n
                variance = sum((v - avg) ** 2 for v in values) / n if n > 1 else 1.0
                std = math.sqrt(variance) if variance > 0 else 1.0
                benchmark[metric] = {'avg': round(avg, 3), 'std': round(std, 3)}

        return benchmark

    @classmethod
    def calculate_ranker_gap(
        cls,
        matches: List[Dict],
        player_performances: List[Dict],
        division: int = 300,
        ranker_api_data: Optional[List[Dict]] = None,
    ) -> Dict[str, Any]:
        """
        ë­ì»¤ ê²©ì°¨ ì¢…í•© ì ìˆ˜ ê³„ì‚°.

        Args:
            matches: Match ë”•ì…”ë„ˆë¦¬ ëª©ë¡
            player_performances: PlayerPerformance ë”•ì…”ë„ˆë¦¬ ëª©ë¡
            division: í˜„ì¬ ë“±ê¸‰ (100=ìŠˆì±”, 200=ì±”, 300=ìŠˆìœ , 400=ìœ )
            ranker_api_data: ranker-stats API ì‘ë‹µ (ì—†ìœ¼ë©´ fallback ì‚¬ìš©)

        Returns:
            {ranker_distance_score, metric_breakdown, division_benchmark, insights, weekly_progress}
        """
        my_stats = cls._compute_my_aggregate_stats(matches, player_performances)
        if not my_stats:
            return cls._empty_result()

        # Get benchmark
        if ranker_api_data:
            api_benchmark = cls._extract_ranker_benchmark_from_api(ranker_api_data)
        else:
            api_benchmark = {}

        # Use API benchmark where available, fallback to division benchmark
        division_key = min(cls.DIVISION_BENCHMARKS.keys(), key=lambda k: abs(k - division))
        division_bench = cls.DIVISION_BENCHMARKS.get(division_key, cls.DIVISION_BENCHMARKS[300])

        metric_breakdown = {}
        z_scores = {}

        for metric, config in cls.METRICS.items():
            if metric not in my_stats:
                continue

            my_val = my_stats[metric]

            if metric in api_benchmark:
                ranker_avg = api_benchmark[metric]['avg']
                ranker_std = api_benchmark[metric]['std']
            else:
                ranker_avg = division_bench.get(metric, my_val)
                ranker_std = cls.RANKER_STD.get(metric, 1.0)

            z = (my_val - ranker_avg) / ranker_std if ranker_std > 0 else 0.0
            z_scores[metric] = z

            # Convert to 0-100 proximity score (50 = average ranker, 100 = far above)
            proximity = 50 + z * 15  # 1 std = 15 points
            proximity = max(0, min(100, proximity))

            metric_breakdown[metric] = {
                'label': config['label'],
                'my_value': round(my_val, 2),
                'ranker_avg': round(ranker_avg, 2),
                'ranker_std': round(ranker_std, 2),
                'z_score': round(z, 2),
                'proximity_score': round(proximity, 1),
                'status': cls._metric_status(z),
                'gap_description': cls._gap_description(metric, my_val, ranker_avg),
            }

        # Weighted overall ranker distance score (0-100)
        total_weight = sum(cls.METRICS[m]['weight'] for m in metric_breakdown)
        if total_weight > 0:
            weighted_proximity = sum(
                metric_breakdown[m]['proximity_score'] * cls.METRICS[m]['weight']
                for m in metric_breakdown
            ) / total_weight
        else:
            weighted_proximity = 50.0

        ranker_distance_score = round(weighted_proximity, 1)

        # Grade label
        grade = cls._distance_grade(ranker_distance_score)

        insights = cls._generate_insights(metric_breakdown, ranker_distance_score, grade, division)

        return {
            'ranker_distance_score': ranker_distance_score,
            'grade': grade,
            'my_stats': my_stats,
            'metric_breakdown': metric_breakdown,
            'division': division,
            'division_label': cls._division_label(division),
            'data_source': 'api' if ranker_api_data else 'fallback',
            'matches_analyzed': len(matches),
            'insights': insights,
        }

    @staticmethod
    def _metric_status(z: float) -> str:
        if z >= 1.0:
            return 'elite'
        elif z >= 0.0:
            return 'above_average'
        elif z >= -0.5:
            return 'average'
        elif z >= -1.0:
            return 'below_average'
        else:
            return 'needs_work'

    @staticmethod
    def _gap_description(metric: str, my_val: float, ranker_avg: float) -> str:
        diff = my_val - ranker_avg
        labels = {
            'avg_rating': ('í‰ì ', 'ì '),
            'win_rate': ('ìŠ¹ë¥ ', '%'),
            'goals_per_game': ('ê²½ê¸°ë‹¹ ë“ì ', 'ê³¨'),
            'shot_accuracy': ('ìŠˆíŒ… ì •í™•ë„', '%'),
            'pass_accuracy': ('íŒ¨ìŠ¤ ì„±ê³µë¥ ', '%'),
            'dribble_success_rate': ('ë“œë¦¬ë¸” ì„±ê³µë¥ ', '%'),
        }
        label, unit = labels.get(metric, (metric, ''))
        if diff >= 0:
            return f"{label}ì´ ë­ì»¤ í‰ê· ë³´ë‹¤ {abs(diff):.1f}{unit} ë†’ìŠµë‹ˆë‹¤"
        else:
            return f"{label}ì´ ë­ì»¤ í‰ê· ë³´ë‹¤ {abs(diff):.1f}{unit} ë‚®ìŠµë‹ˆë‹¤"

    @staticmethod
    def _distance_grade(score: float) -> Dict:
        if score >= 80:
            return {'label': 'ë­ì»¤ê¸‰', 'color': 'gold', 'emoji': 'ğŸ†', 'description': 'ë­ì»¤ì™€ ë™ë“±í•œ ìˆ˜ì¤€'}
        elif score >= 65:
            return {'label': 'ì¤€ë­ì»¤', 'color': 'green', 'emoji': 'â­', 'description': 'ë­ì»¤ì— ê·¼ì ‘í•œ ìˆ˜ì¤€'}
        elif score >= 50:
            return {'label': 'í‰ê· ', 'color': 'blue', 'emoji': 'ğŸ“Š', 'description': 'í‰ê· ì ì¸ ìˆ˜ì¤€'}
        elif score >= 35:
            return {'label': 'ë°œì „ ì¤‘', 'color': 'yellow', 'emoji': 'ğŸ“ˆ', 'description': 'ê°œì„ ì´ í•„ìš”í•œ ìˆ˜ì¤€'}
        else:
            return {'label': 'ì´ˆë³´', 'color': 'red', 'emoji': 'ğŸ®', 'description': 'ê¸°ì´ˆ í›ˆë ¨ì´ í•„ìš”'}

    @staticmethod
    def _division_label(division: int) -> str:
        labels = {100: 'ìŠˆí¼ì±”í”¼ì–¸', 200: 'ì±”í”¼ì–¸', 300: 'ìŠˆí¼ìœ ', 400: 'ìœ ', 500: 'ì„¸ë¯¸í”„ë¡œ', 600: 'í”„ë¡œ'}
        closest = min(labels.keys(), key=lambda k: abs(k - division))
        return labels.get(closest, f'ë“±ê¸‰ {division}')

    @staticmethod
    def _generate_insights(
        breakdown: Dict,
        score: float,
        grade: Dict,
        division: int,
    ) -> List[str]:
        insights = []

        insights.append(
            f"ğŸ¯ ë­ì»¤ê¹Œì§€ì˜ ê±°ë¦¬ ì ìˆ˜: {score:.0f}/100 â€” {grade['label']} {grade['emoji']}"
        )

        # Best metric
        if breakdown:
            best_metric = max(breakdown.items(), key=lambda x: x[1]['z_score'])
            worst_metric = min(breakdown.items(), key=lambda x: x[1]['z_score'])

            bm = best_metric[1]
            if bm['z_score'] > 0:
                insights.append(
                    f"âœ… ê°•ì : {bm['label']} ({bm['my_value']} vs ë­ì»¤ {bm['ranker_avg']})"
                )

            wm = worst_metric[1]
            if wm['z_score'] < -0.5:
                insights.append(
                    f"ğŸ”´ ìµœìš°ì„  ê°œì„  ì˜ì—­: {wm['label']} "
                    f"({wm['my_value']} vs ë­ì»¤ {wm['ranker_avg']})"
                )

        if score >= 65:
            insights.append("ğŸ”¥ ë­ì»¤ ìˆ˜ì¤€ì— ê·¼ì ‘í•˜ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ìˆ˜ì¤€ì„ ìœ ì§€í•˜ê³  ì„¸ë¶€ ê¸°ìˆ ì„ ì—°ë§ˆí•˜ì„¸ìš”!")
        elif score >= 50:
            insights.append("ğŸ“ˆ í‰ê·  ìˆ˜ì¤€ì— ìˆìŠµë‹ˆë‹¤. ì•½ì  ì§‘ì¤‘ í›ˆë ¨ìœ¼ë¡œ ë­ì»¤ì— ë„ì „í•˜ì„¸ìš”!")
        else:
            insights.append("ğŸ’ª ê¸°ë³¸ê¸° ê°•í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤. ìŠˆíŒ…ê³¼ íŒ¨ìŠ¤ ì •í™•ë„ë¶€í„° ê°œì„ í•˜ì„¸ìš”.")

        return insights

    @staticmethod
    def _empty_result() -> Dict[str, Any]:
        return {
            'ranker_distance_score': 0,
            'grade': {'label': 'ë°ì´í„° ì—†ìŒ', 'color': 'gray', 'emoji': 'â“', 'description': ''},
            'my_stats': {},
            'metric_breakdown': {},
            'division': 300,
            'division_label': 'ìŠˆí¼ìœ ',
            'data_source': 'none',
            'matches_analyzed': 0,
            'insights': ['ë¶„ì„í•  ê²½ê¸° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.'],
        }
